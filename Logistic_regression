import numpy as np
import math

class LogisticRegressionFR:
    
    def __init__(self, x, y):
        self.x = np.array(x)
        self.y = np.array(y)
        self.m = x.shape[0]
        self.n = x.shape[1]
        self.w = np.array([])
        self.b = 0
        self.accuracy = 0
    
    def sigmoid(self, z):
        """
        Computes the sigmoid function of a vector z
        """
        g = (np.exp(-z)+1)**-1
        return g
    
    def compute_gradient(self, w, b, lambda_=0):
        """
        Computes the gradient for logistic regression. Divides each gradiend by m.
        Applies l2 regularization via lambda coefficient

        Args:
          X : (ndarray Shape (m,n)) data, m examples by n features
          y : (ndarray Shape (m,))  target value 
          w : (ndarray Shape (n,))  values of parameters of the model      
          b : (scalar)              value of bias parameter of the model
          *argv : unused, for compatibility with regularized version below
        Returns
          dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. 
          dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. 
        """
        dj_dw = np.zeros(w.shape)
        dj_db = 0.

        hx = self.sigmoid(np.dot(self.x, w) + b) # m array
        err = hx - self.y # m array
        dj_db = np.mean(err) # scalar
        dj_dw = np.dot(np.transpose(self.x), err)/self.m # (n,m) x (m,1) = n,1 array
        # apply l2 regularization to w
        dj_dw = dj_dw + lambda_*w/self.m
        
        return dj_db, dj_dw
    
    def compute_cost(self, w, b):
        """
        Computes the mean cost over all examples. Cost function is the log likelihood.
        Args:
          X : (ndarray Shape (m,n)) data, m examples by n features
          y : (ndarray Shape (m,))  target value 
          w : (ndarray Shape (n,))  values of parameters of the model      
          b : (scalar)              value of bias parameter of the model
          *argv : unused, for compatibility with regularized version below
        Returns:
          total_cost : (scalar) cost 
        """
        
        hx = self.sigmoid(np.dot(self.x, w) + b)
        loss = np.multiply(-self.y, np.log(hx)) - np.multiply(1-self.y, np.log(1-hx))
        mean_cost = np.mean(loss)
         
        return mean_cost
    
    def train(self, alpha, num_iters, lambda_=0):
        """
        Performs batch gradient descent to learn theta. Updates theta by taking 
        num_iters gradient steps with learning rate alpha

        Args:
          X :    (ndarray Shape (m, n) data, m examples by n features
          y :    (ndarray Shape (m,))  target value 
          w_in : (ndarray Shape (n,))  Initial values of parameters of the model
          b_in : (scalar)              Initial value of parameter of the model
          gradient_function :          function to compute gradient
          alpha : (float)              Learning rate
          num_iters : (int)            number of iterations to run gradient descent
          lambda_ : (scalar, float)    regularization constant

        Returns:
          w : (ndarray Shape (n,)) Updated values of parameters of the model after
              running gradient descent
          b : (scalar)                Updated value of parameter of the model after
              running gradient descent
        """
        
        w_in = np.zeros(self.n)
        b_in = 0

        # An array to store w's at each iteration primarily for graphing later
        w_history = []
        J_history = []

        for i in range(num_iters):

            # Calculate the gradients
            dj_db, dj_dw = self.compute_gradient(w_in, b_in, lambda_)   

            # Update Parameters w_in and b_in
            w_in = w_in - alpha * dj_dw               
            b_in = b_in - alpha * dj_db

            # Save cost every at intervals 10 times or as many iterations if < 10
            if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):
                w_history.append(w_in)
                J_history.append(self.compute_cost(w_in, b_in))
        
        # Save learned parameters
        self.w = w_in
        self.b = b_in
        
        print('Model trained successfully')
        
        return J_history, w_history
        
    
    def predict(self, x, threshold=0.5):
        """
        Predict whether the label is 0 or 1 using learned logistic
        regression parameters w

        Args:
          x (ndarray (n,1))                : data with n features
          threshold (scalar, default=0.5)  : Probability separating 0 and 1 outputs      

        Returns:
          p (scalar)  : The prediction for x using a threshold
        """
      
        hx = self.sigmoid(np.dot(x, self.w) + self.b)
        if hx >= threshold:
            p = 1
        else:
            p = 0
 
        return p
    
    def compute_accuracy(self, x_test, y_test, threshold=0.5):
        """
        Computes the accuracy of the model given a threshold.
        Predicts 1 if hx = sigmoid(w.x+b) >= threshold,
        predicts 0 otherwise. Saves the accuracy in accuracy attribute
        Args:
         x_test (ndarray (m,n))           : Matrix with test observations
         y_test (ndarray (m,))            : Vector with outputs for each obervation
         threshold (scalar, default=0.5)  : Probability separating 0 and 1 outputs
        """
        p = np.zeros(self.m)
        hx = self.sigmoid(np.dot(x_test, self.w) + self.b)
        p = np.array([1 if i>=threshold else 0 for i in hx])
        self.accuracy = (np.mean(p == y_test) * 100)